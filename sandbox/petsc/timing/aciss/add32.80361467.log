Linear solve converged due to CONVERGED_RTOL iterations 17
KSP Object: 1 MPI processes
  type: bcgs
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 1 MPI processes
  type: asm
    Additive Schwarz: total subdomain blocks = 1, amount of overlap = 1
    Additive Schwarz: restriction/interpolation type - RESTRICT
    Local solve is same for all blocks, in the following KSP and PC objects:
    KSP Object:    (sub_)     1 MPI processes
      type: preonly
      maximum iterations=10000, initial guess is zero
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
      left preconditioning
      using NONE norm type for convergence test
    PC Object:    (sub_)     1 MPI processes
      type: ilu
        ILU: out-of-place factorization
        0 levels of fill
        tolerance for zero pivot 2.22045e-14
        using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
        matrix ordering: natural
        factor fill ratio given 1, needed 1
          Factored matrix follows:
            Mat Object:             1 MPI processes
              type: seqaij
              rows=4960, cols=4960
              package used to perform factorization: petsc
              total: nonzeros=23884, allocated nonzeros=23884
              total number of mallocs used during MatSetValues calls =0
                not using I-node routines
      linear system matrix = precond matrix:
      Mat Object:       1 MPI processes
        type: seqaij
        rows=4960, cols=4960
        total: nonzeros=23884, allocated nonzeros=23884
        total number of mallocs used during MatSetValues calls =0
          not using I-node routines
  linear system matrix = precond matrix:
  Mat Object:   1 MPI processes
    type: seqaij
    rows=4960, cols=4960
    total: nonzeros=23884, allocated nonzeros=23884
    total number of mallocs used during MatSetValues calls =0
      not using I-node routines
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/ibrix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn152 with 1 processor, by bnorris2 Tue Mar  1 16:08:19 2016
Using Petsc Release Version 3.5.3, Jan, 31, 2015 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.039e-01      1.00000   2.039e-01
Objects:              3.000e+01      1.00000   3.000e+01
Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
Flops/sec:            2.457e+07      1.00000   2.457e+07  2.457e+07
Memory:               1.811e+06      1.00000              1.811e+06
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00      0.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.0095e-01  98.6%  5.0086e+06 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

ThreadCommRunKer       1 1.0 5.9605e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie       1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult               35 1.0 3.8826e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  2 30  0  0  0   2 30  0  0  0   386
MatSolve              35 1.0 4.2627e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  2 30  0  0  0   2 30  0  0  0   351
MatLUFactorNum         1 1.0 1.6510e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0    77
MatILUFactorSym        1 1.0 5.1689e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       2 1.0 8.1062e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         2 1.0 5.7316e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetSubMatrice       1 1.0 1.0421e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatGetOrdering         1 1.0 1.0800e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 4.7398e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLoad                1 1.0 2.8294e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 14  0  0  0  0  14  0  0  0  0     0
MatView                9 1.0 5.6713e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
VecDot                34 1.0 1.3254e-03 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  7  0  0  0   1  7  0  0  0   254
VecDotNorm2           17 1.0 3.9244e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   859
VecNorm               19 1.0 2.9135e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0   647
VecCopy                2 1.0 3.5048e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                87 1.0 9.6273e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                1 1.0 4.6015e-05 1.0 9.92e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   216
VecAXPBYCZ            34 1.0 6.2203e-04 1.0 6.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1084
VecWAXPY              34 1.0 5.8007e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   581
VecScatterBegin       70 1.0 1.2937e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
KSPSetUp               2 1.0 5.5504e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 5.1276e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00 25 96  0  0  0  26 96  0  0  0    94
PCSetUp                2 1.0 6.0070e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0    21
PCSetUpOnBlocks        2 1.0 3.5310e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0    36
PCApply               35 1.0 1.8854e-02 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  9 30  0  0  0   9 30  0  0  0    79
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     3              1          752     0
              Matrix     3              2       737324     0
              Vector    13             13       535392     0
      Vector Scatter     1              1          644     0
       Krylov Solver     2              2         2328     0
      Preconditioner     2              2         1960     0
           Index Set     6              6        64224     0
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
-ksp_converged_reason
-ksp_error_if_not_converged 1
-ksp_type bcgs
-ksp_view
-log_summary
-matload_block_size 1
-options_left
-pc_asm_overlap 1
-pc_type asm
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------

Using C compiler: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------

#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
-ksp_converged_reason
-ksp_error_if_not_converged 1
-ksp_type bcgs
-ksp_view
-log_summary
-matload_block_size 1
-options_left
-pc_asm_overlap 1
-pc_type asm
#End of PETSc Option Table entries
There are no unused options.
Linear solve converged due to CONVERGED_RTOL iterations 17
KSP Object: 1 MPI processes
  type: bcgs
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 1 MPI processes
  type: asm
    Additive Schwarz: total subdomain blocks = 1, amount of overlap = 1
    Additive Schwarz: restriction/interpolation type - RESTRICT
    Local solve is same for all blocks, in the following KSP and PC objects:
    KSP Object:    (sub_)     1 MPI processes
      type: preonly
      maximum iterations=10000, initial guess is zero
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
      left preconditioning
      using NONE norm type for convergence test
    PC Object:    (sub_)     1 MPI processes
      type: ilu
        ILU: out-of-place factorization
        0 levels of fill
        tolerance for zero pivot 2.22045e-14
        using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
        matrix ordering: natural
        factor fill ratio given 1, needed 1
          Factored matrix follows:
            Mat Object:             1 MPI processes
              type: seqaij
              rows=4960, cols=4960
              package used to perform factorization: petsc
              total: nonzeros=23884, allocated nonzeros=23884
              total number of mallocs used during MatSetValues calls =0
                not using I-node routines
      linear system matrix = precond matrix:
      Mat Object:       1 MPI processes
        type: seqaij
        rows=4960, cols=4960
        total: nonzeros=23884, allocated nonzeros=23884
        total number of mallocs used during MatSetValues calls =0
          not using I-node routines
  linear system matrix = precond matrix:
  Mat Object:   1 MPI processes
    type: seqaij
    rows=4960, cols=4960
    total: nonzeros=23884, allocated nonzeros=23884
    total number of mallocs used during MatSetValues calls =0
      not using I-node routines
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/ibrix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn147 with 1 processor, by bnorris2 Tue Mar  1 16:08:21 2016
Using Petsc Release Version 3.5.3, Jan, 31, 2015 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.548e-01      1.00000   4.548e-01
Objects:              3.000e+01      1.00000   3.000e+01
Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
Flops/sec:            1.101e+07      1.00000   1.101e+07  1.101e+07
Memory:               1.811e+06      1.00000              1.811e+06
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00      0.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.5329e-01  99.7%  5.0086e+06 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
Linear solve converged due to CONVERGED_RTOL iterations 17
StagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
KSP Object:      %R - percent reductions in this phase
 1 MPI processes
 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
  type: bcgs


  maximum iterations=10000, initial guess is zero
      ##########################################################
      #                                                        #
  left preconditioning
          WARNING!!!                    #
  using PRECONDITIONED norm type for convergence test
         #
      #   This code was compiled with a debugging option,      #
PC Object:      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
  type: asm
 generally two or three times faster.              #
      #                                                        #
        ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
  Additive Schwarz: total subdomain blocks = 1, amount of overlap = 1
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

  Additive Schwarz: restriction/interpolation type - RESTRICT
00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie       1 1.0 1.9073e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  tMult               35 1.0 2.0752e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   722
MatSolve              35 1.0 2.2995e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   652
  Local solve is same for all blocks, in the following KSP and PC objects:
MatLUFactorNum         1 1.0 8.7404e-04 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   146
MatILUFactorSym        1 1.0 3.6597e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       2 1.0 4.2915e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  tAssemblyEnd         2 1.0 3.1400e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 3.0994e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSP Object:MatGetSubMatrice       1 1.0 5.8794e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 5.7817e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  MatIncreaseOvrlp       1 1.0 2.5988e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  tLoad                1 1.0 6.1893e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                9 1.0 1.9812e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 44  0  0  0  0  44  0  0  0  0     0
(sub_)VecDot                34 1.0 8.5926e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   392
  cDotNorm2           17 1.0 2.1839e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1544
  cNorm               19 1.0 1.6761e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1124
VecCopy                2 1.0 3.0041e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
 1 MPI processes
VecSet                87 1.0 5.8246e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  cAXPY                1 1.0 1.1802e-04 1.0 9.92e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    84
VecAXPBYCZ            34 1.0 3.4642e-04 1.0 6.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1947
VecWAXPY              34 1.0 3.1781e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1061
VecScatterBegin       70 1.0 7.0167e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  PSetUp               2 1.0 3.2091e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 3.1229e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00  7 96  0  0  0   7 96  0  0  0   154
  PCSetUp                2 1.0 3.4480e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0    37
PCSetUpOnBlocks        2 1.0 1.9760e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    64
  Apply               35 1.0 1.0578e-02 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  2 30  0  0  0   2 30  0  0  0   142
------------------------------------------------------------------------------------------------------------------------
  Memory usage is given in bytes:

  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
dants' Mem.
Reports information only for process 0.
  
--- Event Stage 0: Main Stage

              Viewer     3              1          752     0
                Matrix     3              2       737324     0
  left preconditioning
 13             13       535392     0
      Vector Scatter     1              1          644     0
       Krylov Solver     2              2         2328     0
      Preconditioner     2              2         1960     0
           Index Set     6              6        64224     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
  -ksp_converged_reason
-ksp_error_if_not_converged 1
PC Object:-ksp_type bcgs
-ksp_view
  -log_summary
  atload_block_size 1
-options_left
(sub_)-pc_asm_overlap 1
-pc_type asm
  nd of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
  nfigure options: --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------
  type: ilu

Using C compiler: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------
  sing include paths: -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------
  
  ETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
lace factorization
-ksp_converged_reason
  -ksp_error_if_not_converged 1
-ksp_type bcgs
  -ksp_view
-log_summary
  -matload_block_size 1
-options_left
  0 levels of fill
-pc_asm_overlap 1
-pc_type asm
#End of PETSc Option Table entries
  ere are no unused options.
    tolerance for zero pivot 2.22045e-14
        using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
        matrix ordering: natural
        factor fill ratio given 1, needed 1
          Factored matrix follows:
            Mat Object:             1 MPI processes
              type: seqaij
              rows=4960, cols=4960
              package used to perform factorization: petsc
              total: nonzeros=23884, allocated nonzeros=23884
              total number of mallocs used during MatSetValues calls =0
                not using I-node routines
      linear system matrix = precond matrix:
      Mat Object:       1 MPI processes
        type: seqaij
        rows=4960, cols=4960
        total: nonzeros=23884, allocated nonzeros=23884
        total number of mallocs used during MatSetValues calls =0
          not using I-node routines
  linear system matrix = precond matrix:
  Mat Object:   1 MPI processes
    type: seqaij
    rows=4960, cols=4960
    total: nonzeros=23884, allocated nonzeros=23884
    total number of mallocs used during MatSetValues calls =0
      not using I-node routines
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/ibrix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn148 with 1 processor, by bnorris2 Tue Mar  1 16:08:21 2016
Using Petsc Release Version 3.5.3, Jan, 31, 2015 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.831e-01      1.00000   6.831e-01
Objects:              3.000e+01      1.00000   3.000e+01
Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
Flops/sec:            7.332e+06      1.00000   7.332e+06  7.332e+06
Memory:               1.811e+06      1.00000              1.811e+06
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00      0.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.8019e-01  99.6%  5.0086e+06 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

ThreadCommRunKer       1 1.0 7.1526e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie       1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult               35 1.0 3.8972e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   384
MatSolve              35 1.0 4.2813e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   350
MatLUFactorNum         1 1.0 1.6539e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    77
MatILUFactorSym        1 1.0 5.2309e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       2 1.0 7.8678e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         2 1.0 5.7411e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetSubMatrice       1 1.0 1.0591e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 1.0829e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 4.5896e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLoad                1 1.0 2.2299e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                9 1.0 2.4483e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 36  0  0  0  0  36  0  0  0  0     0
VecDot                34 1.0 1.3318e-03 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   253
VecDotNorm2           17 1.0 3.9744e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   849
VecNorm               19 1.0 2.9492e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0   639
VecCopy                2 1.0 3.4809e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                87 1.0 9.7108e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                1 1.0 1.3494e-04 1.0 9.92e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    74
VecAXPBYCZ            34 1.0 6.2060e-04 1.0 6.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1087
VecWAXPY              34 1.0 5.9128e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   570
VecScatterBegin       70 1.0 1.2698e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               2 1.0 5.3620e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 5.6109e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00  8 96  0  0  0   8 96  0  0  0    86
PCSetUp                2 1.0 6.0000e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0    21
PCSetUpOnBlocks        2 1.0 3.5291e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0    36
PCApply               35 1.0 1.8892e-02 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  3 30  0  0  0   3 30  0  0  0    79
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     3              1          752     0
              Matrix     3              2       737324     0
              Vector    13             13       535392     0
      Vector Scatter     1              1          644     0
       Krylov Solver     2              2         2328     0
      Preconditioner     2              2         1960     0
           Index Set     6              6        64224     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
-ksp_converged_reason
-ksp_error_if_not_converged 1
-ksp_type bcgs
-ksp_view
-log_summary
-matload_block_size 1
-options_left
-pc_asm_overlap 1
-pc_type asm
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------

Using C compiler: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------

#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
-ksp_converged_reason
-ksp_error_if_not_converged 1
-ksp_type bcgs
-ksp_view
-log_summary
-matload_block_size 1
-options_left
-pc_asm_overlap 1
-pc_type asm
#End of PETSc Option Table entries
There are no unused options.
Linear solve converged due to CONVERGED_RTOL iterations 17
KSP Object: 1 MPI processes
  type: bcgs
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 1 MPI processes
  type: asm
    Additive Schwarz: total subdomain blocks = 1, amount of overlap = 1
    Additive Schwarz: restriction/interpolation type - RESTRICT
    Local solve is same for all blocks, in the following KSP and PC objects:
    KSP Object:    (sub_)     1 MPI processes
      type: preonly
      maximum iterations=10000, initial guess is zero
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
      left preconditioning
      using NONE norm type for convergence test
    PC Object:    (sub_)     1 MPI processes
      type: ilu
        ILU: out-of-place factorization
        0 levels of fill
        tolerance for zero pivot 2.22045e-14
        using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
        matrix ordering: natural
        factor fill ratio given 1, needed 1
          Factored matrix follows:
            Mat Object:             1 MPI processes
              type: seqaij
              rows=4960, cols=4960
              package used to perform factorization: petsc
              total: nonzeros=23884, allocated nonzeros=23884
              total number of mallocs used during MatSetValues calls =0
                not using I-node routines
      linear system matrix = precond matrix:
      Mat Object:       1 MPI processes
        type: seqaij
        rows=4960, cols=4960
        total: nonzeros=23884, allocated nonzeros=23884
        total number of mallocs used during MatSetValues calls =0
          not using I-node routines
  linear system matrix = precond matrix:
  Mat Object:   1 MPI processes
    type: seqaij
    rows=4960, cols=4960
    total: nonzeros=23884, allocated nonzeros=23884
    total number of mallocs used during MatSetValues calls =0
      not using I-node routines
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/ibrix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn132 with 1 processor, by bnorris2 Tue Mar  1 16:08:32 2016
Using Petsc Release Version 3.5.3, Jan, 31, 2015 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.717e-01      1.00000   5.717e-01
Objects:              3.000e+01      1.00000   3.000e+01
Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
Flops/sec:            8.761e+06      1.00000   8.761e+06  8.761e+06
Memory:               1.811e+06      1.00000              1.811e+06
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00      0.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.6887e-01  99.5%  5.0086e+06 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

ThreadCommRunKer       1 1.0 6.9141e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie       1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult               35 1.0 3.8881e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   385
MatSolve              35 1.0 4.2770e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   350
MatLUFactorNum         1 1.0 1.7080e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    75
MatILUFactorSym        1 1.0 5.3501e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       2 1.0 8.1062e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         2 1.0 5.7697e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetSubMatrice       1 1.0 1.0688e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 1.0872e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 4.6015e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Linear solve converged due to CONVERGED_RTOL iterations 17
0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                9 1.0 2.3528e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 41  0  0  0  0  41  0  0  0  0     0
KSP Object:VecDot                34 1.0 1.3149e-03 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   256
 1 MPI processes
     17 1.0 3.9411e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   856
  type: bcgs
         19 1.0 3.0065e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0   627
VecCopy                2 1.0 3.4809e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                87 1.0 9.7275e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   168
VecAXPBYCZ            34 1.0 6.2943e-04 1.0 6.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1072
  left preconditioning
VecWAXPY              34 1.0 5.8699e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   575
  using PRECONDITIONED norm type for convergence test
.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PC Object:             2 1.0 5.5099e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 5.7322e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00 10 96  0  0  0  10 96  0  0  0    84
 1 MPI processes
PCSetUp                2 1.0 6.1140e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0    21
PCSetUpOnBlocks        2 1.0 3.6120e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0    35
  Apply               35 1.0 1.9789e-02 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  3 30  0  0  0   3 30  0  0  0    76
------------------------------------------------------------------------------------------------------------------------
  Additive Schwarz: total subdomain blocks = 1, amount of overlap = 1

  Memory usage is given in bytes:

  Additive Schwarz: restriction/interpolation type - RESTRICT
Object Type          Creations   Destructions     Memory  Descendants' Mem.
  Reports information only for process 0.

--- Event Stage 0: Main Stage

ocks, in the following KSP and PC objects:
              Viewer     3              1          752     0
              Matrix     3              2       737324     0
              Vector    13             13       535392     0
      Vector Scatter     1              1          644     0
       Krylov Solver     2              2         2328     0
      Preconditioner     2              2         1960     0
           Index Set     6              6        64224     0
  ======================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467

-ksp_converged_reason
-ksp_error_if_not_converged 1
  maximum iterations=10000, initial guess is zero
-ksp_view
  -log_summary
-matload_block_size 1
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
-pc_asm_overlap 1
  -pc_type asm
#End of PETSc Option Table entries
  left preconditioning
N kernels
Compiled with full precision matrices (default)
  sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
  Configure options: --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------

Using C compiler: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------
PC Object:
Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------
  
  #PETSc Option Table entries:
(sub_)   /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
  ash 80361467
 1 MPI processes
ason
  sp_error_if_not_converged 1
  sp_type bcgs
  type: ilu
-log_summary
-matload_block_size 1
  -options_left
-pc_asm_overlap 1
e factorization
-pc_type asm
#End of PETSc Option Table entries
There are no unused options.
  0 levels of fill
        tolerance for zero pivot 2.22045e-14
        using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
        matrix ordering: natural
        factor fill ratio given 1, needed 1
          Factored matrix follows:
            Mat Object:             1 MPI processes
              type: seqaij
              rows=4960, cols=4960
              package used to perform factorization: petsc
              total: nonzeros=23884, allocated nonzeros=23884
              total number of mallocs used during MatSetValues calls =0
                not using I-node routines
      linear system matrix = precond matrix:
      Mat Object:       1 MPI processes
        type: seqaij
        rows=4960, cols=4960
        total: nonzeros=23884, allocated nonzeros=23884
        total number of mallocs used during MatSetValues calls =0
          not using I-node routines
 to CONVERGED_RTOL iterations 17
  linear system matrix = precond matrix:
KSP Object:   1 MPI processes
  type: bcgs
 1 MPI processes
  maximum iterations=10000, initial guess is zero
  type: seqaij
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
  left preconditioning
  using PRECONDITIONED norm type for convergence test
  PC Object:  total: nonzeros=23884, allocated nonzeros=23884
  type: asm
  total number of mallocs used during MatSetValues calls =0
overlap = 1
      Additive Schwarz: restriction/interpolation type - RESTRICT
    Local solve is same for all blocks, in the following KSP and PC objects:
    ************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/ibrix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn154 with 1 processor, by bnorris2 Tue Mar  1 16:08:33 2016
Using Petsc Release Version 3.5.3, Jan, 31, 2015 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.010e-01      1.00000   7.010e-01
Objects:              3.000e+01      1.00000   3.000e+01
Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
  Flops/sec:            7.145e+06      1.00000   7.145e+06  7.145e+06
  Memory:               1.811e+06      1.00000              1.811e+06
  maximum iterations=10000, initial guess is zero
  I Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00      0.00000
, divergence=10000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
  left preconditioning
                            and VecAXPY() for complex vectors of length N --> 8N flops
  
Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                          Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
  using NONE norm type for convergence test
 0:      Main Stage: 6.9951e-01  99.8%  5.0086e+06 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
  
------------------------------------------------------------------------------------------------------------------------
  See the 'Profiling' chapter of the users' manual for details on interpreting output.
PC Object:Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
 1 MPI processes
   Reduct: number of global reductions
     Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
        %M - percent messages in this phase     %L - percent message lengths in this phase
    ILU: out-of-place factorization
his phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


  0 levels of fill
      ##########################################################
        #                                                        #
      #                          WARNING!!!                    #
  tolerance for zero pivot 2.22045e-14
      #                                                        #
        #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
  using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
      #   using --with-debugging=no, the performance will      #
        #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                     Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
    factor fill ratio given 1, needed 1
----------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

ThreadCommRunKer       1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie       1 1.0 1.9073e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
    Factored matrix follows:
MatMult               35 1.0 2.0325e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   737
    tSolve              35 1.0 2.2385e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   669
  tLUFactorNum         1 1.0 8.6784e-04 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   147
MatILUFactorSym        1 1.0 3.2592e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
    tAssemblyBegin       2 1.0 2.8610e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         2 1.0 3.1114e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  MatGetRowIJ            1 1.0 1.9073e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
    tGetSubMatrice       1 1.0 5.9605e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 5.7507e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 2.6894e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  MatLoad                1 1.0 1.8840e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
 1 MPI processes
  tView                9 1.0 2.8499e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 41  0  0  0  0  41  0  0  0  0     0
VecDot                34 1.0 7.9560e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   424
    cDotNorm2           17 1.0 2.1720e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1553
VecNorm               19 1.0 1.5783e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1194
VecCopy                2 1.0 2.0027e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  type: seqaij
  cSet                87 1.0 5.4884e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  cAXPY                1 1.0 4.2915e-05 1.0 9.92e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   231
VecAXPBYCZ            34 1.0 3.4475e-04 1.0 6.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1957
VecWAXPY              34 1.0 3.1924e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1057
  VecScatterBegin       70 1.0 6.6304e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
    PSetUp               2 1.0 3.1996e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
rows=4960, cols=4960
  1 1.0 3.0797e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00  4 96  0  0  0   4 96  0  0  0   156
PCSetUp                2 1.0 3.4029e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    37
PCSetUpOnBlocks        2 1.0 1.9712e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    65
  PCApply               35 1.0 9.8891e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   152
  ------------------------------------------------------------------------------------------------------------------------
  
  Memory usage is given in bytes:

  Object Type          Creations   Destructions     Memory  Descendants' Mem.
package used to perform factorization: petsc
  ports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     3              1          752     0
              Matrix     3              2       737324     0
                Vector    13             13       535392     0
        Vector Scatter     1              1          644     0
       Krylov Solver     2              2         2328     0
        Preconditioner     2              2         1960     0
             Index Set     6              6        64224     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
total number of mallocs used during MatSetValues calls =0
-hash 80361467
    sp_converged_reason
  sp_error_if_not_converged 1
-ksp_type bcgs
-ksp_view
  -log_summary
  -matload_block_size 1
  not using I-node routines
  c_asm_overlap 1
-pc_type asm
#End of PETSc Option Table entries
trix:
  Compiled without FORTRAN kernels
  Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes  
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------
   1 MPI processes
: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------
  
Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------
    type: seqaij
  ETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
  -hash 80361467
rows=4960, cols=4960
  sp_converged_reason
-ksp_error_if_not_converged 1
  -ksp_type bcgs
  total: nonzeros=23884, allocated nonzeros=23884
-log_summary
-matload_block_size 1
-options_left
-pc_asm_overlap 1
total number of mallocs used during MatSetValues calls =0
-pc_type asm
    nd of PETSc Option Table entries
There are no unused options.
    not using I-node routines
  linear system matrix = precond matrix:
  Mat Object:   1 MPI processes
    type: seqaij
    rows=4960, cols=4960
    total: nonzeros=23884, allocated nonzeros=23884
    total number of mallocs used during MatSetValues calls =0
      not using I-node routines
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/ibrix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn153 with 1 processor, by bnorris2 Tue Mar  1 16:08:33 2016
Using Petsc Release Version 3.5.3, Jan, 31, 2015 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.697e-01      1.00000   7.697e-01
Objects:              3.000e+01      1.00000   3.000e+01
Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
Flops/sec:            6.507e+06      1.00000   6.507e+06  6.507e+06
Memory:               1.811e+06      1.00000              1.811e+06
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00      0.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.6821e-01  99.8%  5.0086e+06 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

ThreadCommRunKer       1 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie       1 1.0 2.1458e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult               35 1.0 2.0497e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   731
MatSolve              35 1.0 2.2509e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   666
MatLUFactorNum         1 1.0 8.9812e-04 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   142
MatILUFactorSym        1 1.0 3.2592e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       2 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         2 1.0 3.3402e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 3.0994e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetSubMatrice       1 1.0 6.2299e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 6.0105e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 2.6989e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLoad                1 1.0 1.7278e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                9 1.0 3.3669e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 44  0  0  0  0  44  0  0  0  0     0
VecDot                34 1.0 8.1778e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   412
VecDotNorm2           17 1.0 2.1291e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1584
VecNorm               19 1.0 1.5640e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1205
VecCopy                2 1.0 2.0027e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                87 1.0 5.6648e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                1 1.0 4.2915e-05 1.0 9.92e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   231
VecAXPBYCZ            34 1.0 3.4285e-04 1.0 6.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1968
VecWAXPY              34 1.0 3.1924e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1057
VecScatterBegin       70 1.0 6.8784e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               2 1.0 3.2997e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 3.2333e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00  4 96  0  0  0   4 96  0  0  0   149
PCSetUp                2 1.0 3.4530e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    37
PCSetUpOnBlocks        2 1.0 1.9870e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    64
PCApply               35 1.0 1.0021e-02 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   150
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     3              1          752     0
              Matrix     3              2       737324     0
              Vector    13             13       535392     0
      Vector Scatter     1              1          644     0
       Krylov Solver     2              2         2328     0
      Preconditioner     2              2         1960     0
           Index Set     6              6        64224     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
-ksp_converged_reason
-ksp_error_if_not_converged 1
-ksp_type bcgs
-ksp_view
-log_summary
-matload_block_size 1
-options_left
-pc_asm_overlap 1
-pc_type asm
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------

Using C compiler: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------

#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
-ksp_converged_reason
-ksp_error_if_not_converged 1
-ksp_type bcgs
-ksp_view
-log_summary
-matload_block_size 1
-options_left
-pc_asm_overlap 1
-pc_type asm
#End of PETSc Option Table entries
There are no unused options.
Linear solve converged due to CONVERGED_RTOL iterations 17
KSP Object: 1 MPI processes
  type: bcgs
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 1 MPI processes
  type: asm
    Additive Schwarz: total subdomain blocks = 1, amount of overlap = 1
    Additive Schwarz: restriction/interpolation type - RESTRICT
    Local solve is same for all blocks, in the following KSP and PC objects:
    KSP Object:    (sub_)     1 MPI processes
      type: preonly
      maximum iterations=10000, initial guess is zero
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
      left preconditioning
      using NONE norm type for convergence test
    PC Object:    (sub_)     1 MPI processes
      type: ilu
        ILU: out-of-place factorization
        0 levels of fill
        tolerance for zero pivot 2.22045e-14
        using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
        matrix ordering: natural
        factor fill ratio given 1, needed 1
          Factored matrix follows:
            Mat Object:             1 MPI processes
              type: seqaij
              rows=4960, cols=4960
              package used to perform factorization: petsc
              total: nonzeros=23884, allocated nonzeros=23884
              total number of mallocs used during MatSetValues calls =0
                not using I-node routines
      linear system matrix = precond matrix:
      Mat Object:       1 MPI processes
        type: seqaij
        rows=4960, cols=4960
        total: nonzeros=23884, allocated nonzeros=23884
        total number of mallocs used during MatSetValues calls =0
          not using I-node routines
  linear system matrix = precond matrix:
  Mat Object:   1 MPI processes
    type: seqaij
    rows=4960, cols=4960
    total: nonzeros=23884, allocated nonzeros=23884
    total number of mallocs used during MatSetValues calls =0
      not using I-node routines
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/ibrix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn160 with 1 processor, by bnorris2 Tue Mar  1 16:08:34 2016
Using Petsc Release Version 3.5.3, Jan, 31, 2015 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.537e-01      1.00000   4.537e-01
Objects:              3.000e+01      1.00000   3.000e+01
Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
Flops/sec:            1.104e+07      1.00000   1.104e+07  1.104e+07
Memory:               1.811e+06      1.00000              1.811e+06
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00      0.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.5216e-01  99.7%  5.0086e+06 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

ThreadCommRunKer       1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie       1 1.0 2.1458e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult               35 1.0 2.0473e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   732
MatSolve              35 1.0 2.2442e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   668
MatLUFactorNum         1 1.0 8.7500e-04 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   146
MatILUFactorSym        1 1.0 3.1114e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       2 1.0 5.2452e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         2 1.0 3.1519e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetSubMatrice       1 1.0 6.0391e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 5.9104e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 2.5201e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLoad                1 1.0 1.5481e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSP Object:MatView                9 1.0 2.2638e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 50  0  0  0  0  50  0  0  0  0     0
 1 MPI processes
VecDot                34 1.0 7.9775e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   423
VecDotNorm2           17 1.0 2.1005e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1606
VecNorm               19 1.0 1.5712e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1199
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  left preconditioning
7 1.0 6.0940e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  using PRECONDITIONED norm type for convergence test
.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    88
PC Object:            34 1.0 3.3617e-04 1.0 6.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  2007
 1 MPI processes
     34 1.0 3.0947e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1090
  type: asm
gin       70 1.0 6.7496e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               2 1.0 3.3092e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 3.0437e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00  7 96  0  0  0   7 96  0  0  0   158
PCSetUp                2 1.0 3.3400e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0    38
PCSetUpOnBlocks        2 1.0 1.9331e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    66
    Local solve is same for all blocks, in the following KSP and PC objects:
0e+00  2 30  0  0  0   2 30  0  0  0   150
  ----------------------------------------------------------------------------------------------------------------------
  KSP Object:e is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.
(sub_)
--- Event Stage 0: Main Stage

                Viewer     3              1          752     0
KSP Object:   Matrix     3              2       737324     0
   1 MPI processes
tor    13             13       535392     0
        Vector   type: preonly
           1          644     0
       Krylov Solver     2              2         2328     0
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
  maximum iterations=10000, initial guess is zero
1960     0
  left preconditioning
    using PRECONDITIONED norm type for convergence test
   0
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
========================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
  left preconditioning
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
  Additive Schwarz: total subdomain blocks = 1, amount of overlap = 1
  ash 80361467
-ksp_converged_reason
striction/interpolation type - RESTRICT
-ksp_error_if_not_converged 1
  Local solve is same for all blocks, in the following KSP and PC objects:
-ksp_type bcgs
PC Object:  sp_view
-log_summar    -matlo  _block_size 1
  -options_left
  -pc_asm_overlap 1      c_type asm
ses
  #End of PETSc Option Table entries
  Compiled without FORTRAN kernels
  type: preonly
  mpiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
  maximum iterations=10000, initiaConfigure option   --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes  
-  --------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------
    0 levels of fill
tive=1e-05, absolute=1e-50, divergence=10000
AGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------
  sing include paths: -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------
  left preconditioning

  tolerance for zero pivot 2.22045e-14
#PETSc Option Table entries:
    using NONE norm type for convergence test
   /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
  using di-ksp_converged_reason
 to prevent zero pivot [INBLOCKS]
-ksp_error_if_not_converged 1
  (sub_)ype bcgs
  -ksp_view
rdering: natural
  -log_summary
   MPI processes
  atload_block_size 1
-options_left
  factor fill ratio given 1, needed 1
-pc_asm_over  p 1
    c_type asm
  nd of PETSc Option Table entries
There are no unused options.
tion
            0 levels of fill
    Mat Object:    tolerance for zero pivot 2.22045e-14
          near solve converged due to CONVERGED_RTOL iterations 17
  using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
 1 MPI processes
 1 MPI processes
    matrix ordering: natural
  maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
    factor fill ratio given 1, needed 1
  using PRECONDITIONED norm type for convergence test
s 17
  PC Object::   1 MPI processes
ix follows:
    type: asm

  maximum iterations=10000, initial guess is zero
rows=4960, cols=4960
  Additive Schwarz: total subdomain blocks = 1, amount of overla  = 1
  left preconditioning
  using PRE  Additive Schwarz: restriction/interpolati   type - RESTRICT
   Object:    Local solve is same for all blocks, in the following KSP and PC objects:
    pe: asm
  ckage used to perform factorization: petsc
    KSP Object:esses
z: total subdomain blocks = 1, amount of overlap = 1
          ditive Schwarz: restriction/interpolation type - RESTRICT
    (sub_)  Local solve is same for all blocks, in the following KSP and PC objects:
        type: seqaij
 1 MPI processes
  KS  Object:      pe: preonly
total number of mallocs used during MatSetValues calls =0
  (sub_)        maximum iterations=  000, initial guess is zero
       MPI processes
        lerances:  relative=1e-05, absolute=1e-50, divergence=10000
  t using I-node routines
  type: preonly
    left preconditioning
 factorization: petsc
  linear system matrix = precond matrix:
      ximum iterations=10000, initial guess is zero
    using NONE norm type for convergence test
Mat Object:          lerances:  relative=1e-05, absolute=1e-50, divergence=10000
   MPI processes
3884, allocated nonzeros=23884
        ft preconditioning
  type  seqaij
        using NONE norm type for convergence test
total number of m  locs used during MatSetValues calls =0
  ws=4960, cols=4960
          type: il      total: nonzeros=23884, allocated nonzeros=23884
  (sub_)  t using I-node routines
  ILU: out-of-place factorization
    tal number of mallocs used during MatSetValues calls =0
   1 MPI processes
rix = precond matrix:
        0 levels of fill
    type: ilu
not using I-node routines
    linear system matrix = precond matrix:
    Mat Object: for zero pivot 2.22045e-14
    ILU: out-of-place factorization
      type: seqaij
  using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
  rows=4960, cols=4960
  0 levels of fill
    =4960, cols=4960
  tal: nonzeros=23884, allocated nonzeros=23884
  matrix ordering: natural
        tal number of mallocs used during MatSetValues calls =0
  total: nonzeros=23884, allocated nonzeros=23884
    factor fill ratio given 1, needed 1
  t using I-node routines
      using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
************************************************************************************************************************
  ***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
      Factored matrix follows:
*******************************************************************************************
    --------------------------------------------- PETSc Performance Summary: ----------------------------------------------

    brix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn147 with 1 processor, by bnorris2 Tue Mar  1 16:08:35 2016
not using I-node routines
n 3.5.3, Jan, 31, 2015 
  
                         Max       Max/Min        Avg      Total 
    me (sec):           7.844e-01      1.00000   7.844e-01
Objects:              3.000e+01      1.00000   3.000e+01
  Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
Flops/sec:            6.385e+06      1.00000   6.385e+06  6.385e+06
  mory:               1.811e+06      1.00000              1.811e+06
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
  MPI Reductions:       0.000e+00      0.00000
    Factored matrix follows:
flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops
  
Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
  :      Main Stage: 7.8294e-01  99.8%  5.0086e+06 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
  tal number of mallocs used during MatSetValues calls =0
---------------------------------------------------------------
  See the 'Profiling' chapter of the users' manual for details on interpreting output.
  Phase summary info:
Mat Object:umber of times phase was executed
   Time and Flops: Max - m  imum over all processors
                   Ratio - ratio of maximum to minimum over all processors
    type: seqaij
f messages sent
  **********************************************************************************************************************
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
0 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
   Global: entire computation
******************************************************************************************
     Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
--------------------------

/i      %T - percent time in this phase         %F - percent flops in this phase
rch-linux2-c-mpich3.1-gcc4.9 named cn152 with 1 processor, by bnorris2 Tue Mar  1 16:08:35 2016
   MPI processes
se Version 3.5.3, Jan, 31, 2015 
 percent message lengths in this phase
                          Max       Max/Min        Avg      Total 
Time (sec):           8.535e-01      1.00000   8.535e-01
rs)/(max time over all processors)
--Objects:              3.000e+01      1.00000   3.000e+01
-------------------------------------------------------------
  Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
  ws=4960, cols=4960
 5.868e+06      1.00000   5.868e+06  5.868e+06
Memory:               1.811e+06      1.00000              1.811e+06
      #                          WARNING!!!                    #
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
      #                                                        #
      #   This code was compiled with a debugging option,      #
00
  type: seqaij
       0.000e+00      0.00000
re                #

Flop counting convention: 1 flop = 1 real number operation of ty   (multiply/divide/add/subtract)
      #   be generally two or three times faster.              #
      #                                                        #
ength N --> 2N flops
      ##########################################################


ngth N --> 8N flops
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total                    Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
package used to perform factorization: petsc
---------------------------------------------------------------------------
  -- Event Stage 0: Main Stage

-----------------------------------------------------------------------------------------
    readCommRunKer       1 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie    rows=4960, cols=4960
.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult               35 1.0 2.0614e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   727
  tSolve              35 1.0 2.2454e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   667
  tLUFactorNum         1 1.0 8.6284e-04 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   148
  MatILUFactorSym        1 1.0 3.12  e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       2 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         2 1.0 3.0518e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
total: nonzeros=23884, allocated nonzeros=23884
e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
   0  0  0  0  0     0
  MatGetOrdering         1 1.0 5.8508e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Ma      %M - percent messages in this phase     %L - percent message lengths in this phase
0  0   0  0  0  0  0     0
package used to perform factorization: petsc
.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
  0  55  0  0  0  0     0
  ----------------------------------------------------------------------------------------------------------------------  
Ve

otNorm2           17 1.0 2.1029e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1604
        ##########################################################
0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1134
Ve  opy                2 1.0 1.9073e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
total number of mallocs used during MatSetValues calls =0
     #
0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
    cAXPY                1 1.0 1.1206e-04 1.0 9.92e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    89
VecAXPBYCZ            34 1.0 3.4142e-04 1.0 6.75e+05 1.0 0.0e+00   0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1976
      #   To get timing results run ./configure                #
0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1078
  total: nonzeros=23884, allocated nonzeros=23884
ance will      #
0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
      #   be generally two or three times faster.              #
0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
      #                                                        #
0.0e+00 0.0e+00  4 96  0  0  0   4 96  0  0  0   159
    SetUp                2 1.0 3.2840e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    39
  PCSetUpOnBlocks        2 1.0 1.9140e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    67

                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
PCApply               35 1.0 1.0006e-02 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   150
  ----------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
  t using I-node routines
tage

ThreadCommRunKer       1 1.0 7.1526e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ObThreadCommBarrie       1 1.0 3.8147e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  ports information only for process 0.
MatMult               35 1.0 3.8981e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   384

--- Event Stage 0: Main Stage

total number of mallocs used during MatSetValues calls =0
.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   351
              Viewer     3              1          752     0
              Matrix     3              2       737324     0
+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    76
                Vector    13             13       535392     0
+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
        Vector Scatter     1              1          644     0
+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
         Krylov Solver     2              2         2328     0
+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 5.9605e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  tGetSubMatrice       1 1.0 1.0591e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 1.0841e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
==
MatIncreaseOvrlp       1 1.0 4.6301e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  ETSc Option Table entries:
1.2460e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
   MPI processes
      9 1.0 4.4831e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 53  0  0  0  0  53  0  0  0  0     0
-hash 80361467
       34 1.0 1.3189e-03 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   256
VecDotNorm2           17 1.0 4.0889e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   825
  cNorm               19 1.0 3.0661e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0   615
-ksp_type bcgs
  cCopy                2 1.0 3.4094e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                87 1.0 9.6035e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  linear system matrix = precond matrix:
VecAXPY                1 1.0 1.2708e-04 1.0 9.92e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    78
-matload_block_size 1
  cAXPBYCZ            34 1.0 6.2132e-04 1.0 6.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1086
-options_left
  c_asm_overlap 1
    34 1.0 5.8389e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   578
-pc_type asm
in       70 1.0 1.2944e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  #End of PETSc Option Table entries
e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MaCompiled without FORTRAN kernels
47e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00  6 96  0  0  0   6 96  0  0  0    89
  ws=4960, cols=4960
ecision matrices (default)
e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0    21
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
  0  0  0    36
  Configure options: --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes  
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------
 1 MPI processes
: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------
  mory usage is given in bytes:

norris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------
  tal: nonzeros=23884, allocated nonzeros=23884
f90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------
Reports information only for process 0.
  #PETSc Option Table entries:
e

  type: seqaij
ris2/UFloridaSparseMat/petsc/add32.petsc
   0
  ash 80361467
atrix     3              2       737324     0
-ksp_converged_reason
  13             13       535392     0
    sp_error_if_not_converged 1
          1          644     0
-ksp_type bcgs
olver     2              2         2328     0
      Preconditioner     2              2         1960     0
           In  x Set     6              6        64224     0
======================  ================================================================================================
Average time to get PetscTime(): 9.53674e-08
  ETSc Option Table entries:
  c_type asm
orris2/UFloridaSparseMat/petsc/add32.petsc
#End of PETSc Option Table entries
  ere are no unused options.
  linear system matrix = preco   matrix:
-ksp_type bcgs
  tal: nonzeros=23884, allocated nonzeros=23884
-ksp_view
Mat Object:y
-matload_block_size 1
  -options_left
 1 MPI processes

  c_type asm
#End of PETSc Option Table entries
total number ofCompiled without FORTRAN kernels
 calls =0
  mpiled with full precision matrices (default)
  zeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
rows=4960, cols=4960
download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------
  
Using C compiler: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------
  
Using include paths: -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------
not using I-node routines
ng Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------
  linear system matrix = precond matrix:
  ETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
Mat Object:r of mallocs used during MatSetValues calls =0
-hash 80361467
-ksp_converged_reason
 1 MPI processes
_converged 1
-ksp_type bcgs
    type: senot u-log_summary
outines
  -matload_block_size 1
-options_left
rows=4960, cols=4960
***************************************************************************************************
  *             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
***********************************  ***********************************************************************************
There are no unused options.

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/ibrix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn126 with 1 processor, by bnorris2 Tue Mar  1 16:08:35 2016
  ing Petsc Release Version 3.5.3, Jan, 31, 2015 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.154e+00      1.00000   1.154e+00
Objects:              3.000e+01      1.00000   3.000e+01
Flops:                5.009e+06      1.00000   5.009e+06  5.009e+06
Flops/sec:            4.339e+06      1.00000   4.339e+06  4.339e+06
Memory:               1.811e+06      1.00000              1.811e+06
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00      0.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
*********************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
**
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
---

/ibrix/home11/bnorris2/research/lighthouse/sandbox/petsc/new/solvers-aciss on a arch-linux2-c-mpich3.1-gcc4.9 named cn130 with 1 processor, by bnorris2 Tue Mar  1 16:08:35 2016

------------------------------------------------------------------------------------------------------------------------
Using Petsc Release Version 3.5.3, Jan, 31, 2015 
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
      Max       Max/Min        Avg      Total 
Time (sec):           1.189e+00      1.00000   1.189e+00
Objects:              3.000e+01      1.00000   3.000e+01
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
    1.00000   5.009e+06  5.009e+06
Flops/sec:            4.213e+06      1.00000   4.213e+06  4.213e+06
Memory:               1.811e+06      1.00000              1.811e+06
   Global: entire computation
MPI Messages:         0.000e+00      0.00000   0.000e+00  0.000e+00
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
MPI Reductions:       0.000e+00      0.00000
 %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
tract)
------------------------------------------------------------------------------------------------------------------------
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
      #                                                        #
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
      #                          WARNING!!!                    #
 0:      Main Stage: 1.1872e+00  99.9%  5.0086e+06 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
      #                                                        #
      #   This code was compiled with a debugging option,      #
--------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
th-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


                   Ratio - ratio of maximum to minimum over all processors
      --- Global ---  --- Stage ---   Total
   Mess: number of messages sent
     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
   Avg. len: average message length (bytes)
----------------------------------------------------------------------------
   Reduct: number of global reductions
   Global: entire computation
.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie       1 1.0 1.9073e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
0  0 30  0  0  0   0 30  0  0  0   734
      %M - percent messages in this phase     %L - percent message lengths in this phase
  0  0   0 30  0  0  0   667
      %R - percent reductions in this phase
1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   123
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------
MatAssemblyEnd         2 1.0 3.0398e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0


      ##########################################################
0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
      #                                                        #
0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 5.8103e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
      #                          WARNING!!!                    #
      #                                                        #
0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
      #   This code was compiled with a debugging option,      #
0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
      #   To get timing results run ./configure                #
0.0e+00 0.0e+00 53  0  0  0  0  53  0  0  0  0     0
      #   using --with-debugging=no, the performance will      #
0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   414
      #   be generally two or three times faster.              #
0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1570
VecNorm               19 1.0 1.6379e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1151
      #                                                        #
VecCopy                2 1.0 2.0027e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
      ##########################################################


VecSet                87 1.0 5.4860e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

114e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1084
ThreadCommRunKer       1 1.0 3.8147e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               2 1.0 3.2330e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ThreadCommBarrie       1 1.0 1.9073e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 3.9112e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00  3 96  0  0  0   3 96  0  0  0   123
MatMult               35 1.0 2.0368e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   736
MatSolve              35 1.0 2.2480e-03 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0   666
MatLUFactorNum         1 1.0 8.6808e-04 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   147
MatILUFactorSym        1 1.0 3.1805e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------
MatAssemblyBegin       2 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         2 1.0 3.1590e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Memory usage is given in bytes:

MatGetRowIJ            1 1.0 2.8610e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Object Type          Creations   Destructions     Memory  Descendants' Mem.
MatGetSubMatrice       1 1.0 6.0105e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 5.7197e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 0: Main Stage

MatIncreaseOvrlp       1 1.0 2.6703e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
              Viewer     3              1          752     0
MatLoad                1 1.0 1.7130e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
              Matrix     3              2       737324     0
MatView                9 1.0 5.9684e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 50  0  0  0  0  50  0  0  0  0     0
              Vector    13             13       535392     0
VecDot                34 1.0 8.3065e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   406
      Vector Scatter     1              1          644     0
VecDotNorm2           17 1.0 2.1458e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1572
       Krylov Solver     2              2         2328     0
VecNorm               19 1.0 1.6618e-04 1.0 1.88e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1134
      Preconditioner     2              2         1960     0
           Index Set     6              6        64224     0
+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
========================================================================================================================
VecAXPY                1 1.0 4.1008e-05 1.0 9.92e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   242
VecAXPBYCZ            34 1.0 3.4523e-04 1.0 6.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 13  0  0  0   0 13  0  0  0  1954
VecWAXPY              34 1.0 3.2401e-04 1.0 3.37e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1041
VecScatterBegin       70 1.0 6.8879e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
-ksp_converged_reason
KSPSetUp               2 1.0 3.0994e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
-ksp_error_if_not_converged 1
KSPSolve               1 1.0 3.2286e-02 1.0 4.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00  3 96  0  0  0   3 96  0  0  0   149
-ksp_type bcgs
-ksp_view
             2 1.0 3.3460e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    38
PCSetUpOnBlocks        2 1.0 1.9250e-03 1.0 1.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0    66
PCApply               35 1.0 1.0465e-02 1.0 1.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1 30  0  0  0   1 30  0  0  0   143
------------------------------------------------------------------------------------------------------------------------
-pc_asm_overlap 1
-pc_type asm
is given in bytes:

#End of PETSc Option Table entries
structions     Memory  Descendants' Mem.
Compiled without FORTRAN kernels
ess 0.

--- Event Stage 0: Main Stage

rices (default)
              Viewer     3              1          752     0
sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------

Using C compiler: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------
       Krylov Solver     2              2         2328     0
ch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------
           Index Set     6              6        64224     0
========================================================================================================================
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-ksp_error_if_not_converged 1
-ksp_type bcgs
reason
-ksp_view
_if_not_converged 1
-log_summary
s
-matload_block_size 1
-options_left
-pc_asm_overlap 1
e 1
-pc_type asm

#End of PETSc Option Table entries
There are no unused options.
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-hypre=yes --COPTFLAGS="-g -O3" --FOPTFLAGS="-g -O3" --FFLAGS="-g -O3" --CFLAGS="-g -O3" --download-superlu=yes
-----------------------------------------
Libraries compiled on Fri Nov 27 17:45:54 2015 on cn164 
Machine characteristics: Linux-2.6.32-358.23.2.el6.x86_64-x86_64-with-redhat-6.6-Santiago
Using PETSc directory: /home11/bnorris2/petsc/petsc-3.5.3
Using PETSc arch: arch-linux2-c-mpich3.1-gcc4.9
-----------------------------------------

Using C compiler: mpicc -g -O3 -fPIC -g -O3  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -g -O3 -fPIC -g -O3   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/include -I/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/include -I/packages/mpich2/3.1.4_gcc-4.9.2/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lpetsc -Wl,-rpath,/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -L/ibrix/users/home11/bnorris2/petsc/petsc-3.5.3/arch-linux2-c-mpich3.1-gcc4.9/lib -lsuperlu_4.3 -lHYPRE -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -lmpicxx -lstdc++ -llapack -lblas -lX11 -lssl -lcrypto -lpthread -lmpifort -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpicxx -lstdc++ -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -L/packages/mpich2/3.1.4_gcc-4.9.2/lib -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -L/ibrix/packages/gcc/4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib/gcc -L/ibrix/packages/gcc/4.9.3/lib/gcc -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib64 -L/ibrix/packages/gcc/4.9.3/lib64 -Wl,-rpath,/ibrix/packages/gcc/4.9.3/lib -L/ibrix/packages/gcc/4.9.3/lib -ldl -Wl,-rpath,/packages/mpich2/3.1.4_gcc-4.9.2/lib -lmpi -lgcc_s -ldl  
-----------------------------------------

#PETSc Option Table entries:
-f /home11/bnorris2/UFloridaSparseMat/petsc/add32.petsc
-hash 80361467
-ksp_converged_reason
-ksp_error_if_not_converged 1
-ksp_type bcgs
-ksp_view
-log_summary
-matload_block_size 1
-options_left
-pc_asm_overlap 1
-pc_type asm
#End of PETSc Option Table entries
There are no unused options.
