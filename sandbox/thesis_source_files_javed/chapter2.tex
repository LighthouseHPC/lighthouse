\chapter{Background}
\label{backgroundchapter}

A multitude of numerical and tuning software is available for matrix algebra computations and their optimization. In this chapter, we survey a variety of such software, providing examples that might be appropriate for the Lighthouse taxonomy. Section 2.1 reviews the support routine libraries that provide support for basic linear algebra computations. Section 2.2 mentions various direct solver packages for dense matrices. Section 2.3 and section 2.4 talk about the direct and iterative solver packages for sparse matrix algebra computations respectively. Section 2.5 briefly reviews the software libraries that are used for preconditioning sparse matrices. Sections 2.6 and 2.7 discuss various optimization tools and domain specific compilers for numerical linear algebra computations.

\section{Support routines}
There are many support routine packages available for basic vector and matrix operations. Some of the widely used support routine packages are reviewed in the following subsections.

\subsection{BLAS: Basic Linear Algebra Subprograms}
The BLAS \cite{dongarra} are Fortran77 routines for performing basic vector and matrix operations. The Level 1 BLAS perform scalar, vector and vector-vector operations, the Level 2 BLAS perform matrix-vector operations, and the Level 3 BLAS perform matrix-matrix operations. The BLAS are commonly used in the development of other software packages for higher level matrix algebra operations. 
\subsection{Vendor-supplied and other support routine libraries}
BLAS-based vendor-supplied dense matrix algebra libraries include AMD's ACML \cite{amd}, Apple's Accelerate Framework \cite{apple}, IBM's ESSL \cite{ibm}, HP's MLIB \cite{hp}, Intel's MKL \cite{intel}, Oracle's Sun Performance Library \cite{sun}, Cray's LibSci \cite{libsci} and so on. uBLAS \cite{ublas} is a C++ library that offers 3 levels of BLAS functionality for dense, packed and sparse matrices. Blitz++ \cite{blitz} is a C++ class library for scientific computing that provides dense arrays and vectors, random number generators, and small vectors for representing vector fields. The Template Numerical Toolkit (TNT) \cite{tnt} is a collection of interfaces and reference implementations of numerical objects useful for scientific computing in C++. TNT defines interfaces for basic data structures, such as multidimensional arrays and sparse matrices, commonly used in numerical applications.

\section{Dense direct solver packages}
One of the most popular dense direct solver packages is LAPACK (Linear Algebra PACKage) \cite{bai}, which provides routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations, eigenvalue  problems, singular value problems and more. Among the other dense linear algebra solvers are Pliris \cite{pliris}, FLENS \cite{flens}, Elemental \cite{elemental} and rejtrix \cite{rejtrix}. A number of dense linear algebra libraries have been developed for multicore architectures. For example, Matrix Algebra on GPU and Multicore Architectures (MAGMA) \cite{magma}, Parallel Linear Algebra Package (PLAPACK) \cite{plapack}, Parallel Linear Algebra for Scalable Multi-core Architectures (PLASMA) \cite{plasma}, Scalable Linear Algebra PACKage (ScaLAPACK) \cite{scalapack} and PRISM \cite{prism}.

\section{Sparse direct solver packages}
A large number of direct solvers are available for sparse matrix algebra computation. NIST S-BLAS provides a library of basic routines for performing sparse linear algebra operations and includes support for all four precision types (single, double precision real and complex) and Level 1, 2, and 3 operations \cite{duff}. SPARSE \cite{sparse}, SPOOLES \cite{spooles} are libraries for solving sparse real and complex linear systems of equations. In addition to that, SPARSE can also be used to solve transposed systems, find determinants, and estimate errors caused by ill-conditioned systems of equations and instability in the computations. SparseLib++ \cite{sparselib} is another C++ class library for platform independent efficient sparse matrix computations. CHOLMOD \cite{cholmod} is a collection of ANSI C routines that provides support for sparse Cholesky factorization and updating/downdating. DSPACK \cite{dspack} uses direct methods on multiprocessors and networks for solving sparse linear systems. HSL \cite{hsl} is a set of state-of-the-art packages for solving sparse linear systems of equations and eigenvalue problems. MUMPS (MUltiforntal Massively Parallel sparse direct Solver) \cite{mumps} can be used for solving large linear systems, parallel factorization, iterative refinement, backward error analysis, partial factorization, Schur complement matrix and so on. PSPASES (Parallel SPArse Symmetric dirEct Solver) \cite{pspases} is a high performance, scalable, parallel, MPI-based library that provides support for solving linear systems of equations involving sparse symmetric positive definite matrices. A general purpose library is SuperLU \cite{superlu}. It performs LU decomposition with partial pivoting and solves the triangular system through forward and backward substitution. TAUCS \cite{taucs} provides support for various factorizations, iterative solvers, matrix operations, matrix generators, preconditioning and so on. Amesos \cite{amesos} is a direct solver package that attempts to make solving a system of linear equations as easy as possible. UMFPACK \cite{umfpack} is a collection of routines that uses Unsymmetric MultiFrontal method to solve unsymmetric sparse linear systems. y12m \cite{y12m} is a FORTRAN subroutine for solving sparse systems of linear equations by Gaussian elimination.

\section{Sparse iterative solver packages}
PETSc \cite{petsc} is a collection of data structures and routines that provides scalable methods for preconditioning and solving sparse linear and nonlinear systems. ViennaCL \cite{viennacl} focuses on common linear algebra operations and solving large systems of equations using iterative methods. AGMG (AGregation-based algebraic MultiGrid) \cite{agmg} offers an efficient method for solving large systems arising from the discretization of scalar second order elliptic PDEs. BILUM \cite{bilum} uses Krylov subspace methods preconditioned by some multi-level block ILU preconditioning techniques to solve general sparse linear systems. BlockSolve95 \cite{blocksolve95} is a package of routines that can used for solving large sparse symmetric systems on massively parallel distributed memory systems and networks of workstations. SPARSKIT \cite{sparskit} and Iterative Template Library (ITL) \cite{itl} are generic iterative solvers for sparse matrix computation. Sparse Linear Algebra Package (SLAP) \cite{slap} and IML++ (Iterative Methods Library) \cite{pozo} provide iterative methods for solving large sparse symmetric and nonsymmetric linear systems of equation. Parallel Iterative Methods (PIM) \cite{pim} contains Fortran 77 routines that use various iterative methods for solving systems of linear equations in a parallel computing environment. Researchers at CERFACS have developed a set of routines for real and complex, single and double precision arithmetic designed for serial, shared and distributed memory computers \cite{cerfacs}. pARMS (parallel Algebraic Recursive Multilevel Solvers) \cite{parms} is a collection of parallel solvers that rely on a Recursive Multi-level ILU factorization for solving distributed sparse linear systems of equations. A scalable parallel library - Lis (Library of Iterative Solvers) \cite{lis} uses iterative methods to solve linear equations and standard eigenvalue problems with real sparse matrices. A scientific library called HIPS (Hierarchical Iterative Parallel Solver) \cite{hips} offers an efficient parallel iterative solver for very large sparse linear systems. ITPACK \cite{itpack} is a set of four packages for solving large sparse linear systems. Iterative Solvers (ITSOL) \cite{itsol} package provides a library of iterative solvers for general sparse linear systems of equations.Trilinos \cite{trilinos} is a software framework for the solution of large-scale, complex multi-physics engineering and scientific problems. Trilinos is unique for its focus on software packages. Belos \cite{belos} is a Trilinos package that provides iterative linear solvers and a great linear solver developer framework. Komplex \cite{komplex} is another Trilinos package that contains solvers for complex valued linear systems.

\section{Preconditioners}
Many preconditioners have been developed for conditioning problems into a form that is more suitable for numerical solution. ILUPACK \cite{ilupack}, which is based on inverse-based ILUs, can be used for conditioning general real and complex matrices and real and complex symmetric positive definite systems. BPKIT \cite{bpkit} is a toolkit that provides block preconditioners. MLD2P4 \cite{mld2p4} is a package of parallel algebraic multi-level preconditioners. AztecOO, IFPACK, IFPACK2, ML, Teko are some Trilinos packages that provide a variety of preconditioners \cite{trilinos}. Hypre library \cite{hypre} has several families of high performance preconditioned algorithms that are focused on the scalable solution of very large sparse linear systems. MSPAI (Modified Sparse Matrix Inverse) \cite{mspai} preconditioner is an extended version of the SPAI \cite{spai} preconditioner. FSPAI \cite{fspai} is a preconditioner for large sparse and ill-conditioned symmetric positive definite systems of linear equations.

\section{Code optimization tools}
There are various tools available for optimizing BLAS routines. GotoBLAS2 \cite{gotoblas2} uses new algorithms and memory techniques to optimize performance of the BLAS routines. Vendor supplied BLAS libraries \cite{amd,apple,intel,hp,sun,ibm} also provide highly optimized BLAS routines. OpenBLAS \cite{openblas} is based on GotoBLAS2 \cite{gotoblas2} and provides an optimized BLAS library for new architectures. Further performance improvement is possible using various techniques. An expert can manually transform the code by unrolling loops, blocking for multiple levels of cache, inserting prefetch instructions. However, this is a time consuming process and requires high level of expertise. It reduces readability, maintainability and performance portability of the code \cite{hoisie}. Using the appropriate tuned libraries can greatly improve performance and does not require complex programming by the user. But these libraries often provide limited functionality. ATLAS (Automatically Tuned Linear Algebra Software) \cite{whaley} provides portable optimal linear algebra software. It offers a complete BLAS API and a small subset of the LAPACK API and can achieve performance comparable to machine-specific tuned libraries. Using PHiPAC (Portable High Performance ANSI C) \cite{phipac} it is possible to automatically reach very high performance on BLAS 3 routines for matrix-matrix operations for a given architecture/compiler. Active Harmony \cite{activeharmony} is a software architecture under development that focuses on adapting to heterogeneous and changing environments. It supports distributed execution of computational objects through dynamic a execution environment, automatic application adaptation and shared-data interfaces. Orio \cite{orio} uses annotated source code to optimize low level performance of a fragment of code. It tunes the same operation using different optimization parameters, generating many versions of the same piece of code. Then it selects the best among the different versions of the tuned code by performing an empirical search. The Berkeley Benchmarking and Optimization (BeBOP) \cite{bebop} Group is working on automating the process of performance tuning, specifically the computational kernels that are widely used in scientific computation and information retrieval.

Many rudimentary resource-intensive scientific computing tasks, for example solving linear systems, can be done using many different algorithms. The selection of the right algorithms that are well suited for the task at hand and machine architecture is crucial. Selecting appropriate algorithms can be difficult because of the sheer number of algorithmic choices. Self Adapting Large-scale Solver Architecture (SALSA) \cite{fuentes,salsa} tries to facilitate the process of finding suitable linear and nonlinear system solvers using statistical techniques such as principal component analysis. SALSA uses an automated data analyzer to find out unnecessary information about the structure of input data, then it creates a data model to express the information as structured metadata and finally, with a self-adapting decision engine, it combines the metadata and other information such as its history of earlier performances to select the best library and algorithm for the problem. SALSA has a history database, a meta-data vocabulary and an analysis module to aid this selection process. Over time a SALSA system becomes more intelligent by learning from previous performances, tuning its heuristics and coming up with new ones. A different approach is taken by NetSolve \cite{netsolve} which helps the users to solve complex scientific problems remotely through a client-server system. It lets the users access both hardware and software resources that are distributed over a network. Searching through the computational resources available on a network, NetSolve picks the best one to solve the problem and returns the solution to the user. Since NetSolve provides only the solution to the problem instead of an optimized implementation of it, users looking for optimized implementations of the algorithms to solve the problem have little or no use of the system.

\section{Domain-specific compilers}
Domain-specific compilers and special purpose languages can provide more options for optimization of matrix algebra algorithms. MAJIC (MATLAB Just-In-Time Compiler) \cite{menon} employs a mathematical framework in order to exploit the semantic properties of matrix operations in loop-based languages such as MATLAB and FORTRAN. The Broadway compiler \cite{guyer} allows automatic customization of software libraries to increase portability and efficiency across different hardware and software environments. The Formal Linear Algebra Methods Environment (FLAME) \cite{flame} attempts to simplify the development of dense linear algebra libraries by incorporating a new notation for expressing algorithms, a method of systematic derivation of algorithms, APIs, and tools for automatic derivation, implementation and analysis of algorithms and implementations. The Build to Order BLAS (BTO) \cite{bto} compiler uses a scalable search algorithm to select the best combination of loop fusion, array contraction and multi-threading to achieve high performance.

Tuning tools of these sorts are usually designed and developed by compiler researchers. As a result, the interfaces are based on concepts not accessible to most computational scientists. The existing annotation languages and syntaxes for transformations such as loop unrolling, cache-blocking are very complicated and result in steep learning curves.